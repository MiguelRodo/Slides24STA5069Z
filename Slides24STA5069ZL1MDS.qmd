---
title: Multidimensional scaling (MDS)
subtitle: Visualising distance data
author: Francesca Little (reformatted and edited by Miguel Rodo)
date: "2024-02-12"
bibliography: zotero.bib
format:
  beamer:
    embed-resources: true
    aspectratio: 169
    urlcolor: cyan
    linkcolor: blue
    filecolor: magenta
    include-in-header:
      file: preamble.tex
---

## Key references

- AJ Izenman, “Modern Multivariate Statistical Techniques”, Ch 13, Springer, 2013.
- Hastie, Trevor, Robert Tibshirani, and J. H. (Jerome H.) Friedman. “The Elements of Statistical Learning : Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics”, Section 14.8. New York: Springer, 2009.

## Example: representing cities' relative locations by airline distance

```{r}
#| results: asis
#| fig-align: center
knitr::include_graphics(
  projr::projr_path_get("data-raw-img", "airline_distance.png")
)
```

```{r}
#| include: false
library(ggplot2)
library(cowplot)
library(tibble)
library(ggrepel)
```

\addtocounter{framenumber}{-1}

## Two-dimensional MDS map

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{_data_raw/img/airline_mds.png}
\end{figure}

## What is MDS?

- Goal:
  - Represent distance-data in a low-dimensional space, typically for visualization.
  
\pause

- Contrast with typical dimensionality reduction techniques:
  - Whilst other such techniques, e.g. PCA, begin with the original data points (i.e. $\symbf{X}$), MDS begins with the dissimilarities or similarities between points.

\pause

- MDS is a family of techniques, differing by:
  - Valid interpretations of distances between points in generated map (ratio, interval or rank)
  - Optimisation function
  - Method of optimisation (eigen-decomposition vs numerical)
  - Weights of per-distance errors

## Scales of measurement

- Stevens (1946) proposed four scales of measurement:
  - Ratio: a natural zero exists, meaning ratios can be meaningfully defined.
    - Example: height.
    - Non-example: temperature.
  - Interval: no natural exists, but the differences between points is directly comparable.
    - Example: temperature.
    - Non-example: rank-based judgements.
  - Ordinal: points have a greater than/less than relationship to one another, but the differences between points are not necessarily directly comparable.
    - Example: rank-based judgements.
    - Non-example: someone’s name.
  - Nominal: no greater than/less than relationship between points.
    - Example: people’s names.

## Categories of MDS techniques

- Metric:
  - Aim to represent actual distances.
  - Typically, distances may be interpreted on at least the interval scale.
  - Sub-techniques:
    - Classical scaling
    - Least-squares scaling
- Non-metric:
  - Aim to preserve ranks.
  - Distances only interpretable in an ordinal sense.

## Input data

:::: {.columns}

::: {.column width="50%"}

### Similarity/dissimilarity measure 

- *Purpose*:
  - Captures differences between two observations
- *Properties*:
  - Symmetric: Dissimilarity from object $A$ to $B$ the same as dissimilarity from object $B$ to $A$
  - Each object has zero distance from itself
  - Typically: range does not cross zero
- *Notation*:
  - Dissimilarity from $i$-th object to $j$-th object: $\delta_{ij}$

:::

::: {.column width="50%"}
### Proximity matrix

- *Nature*:
  - Matrix whose $i$-th, $j$-th element is $\delta_{ij}$
- *Implied properties*:
  - Symmetric
  - Hollow (0s on diagonal)
  - Square ($n \times n$)
  - Typically, either non-negative or non-positive
- *Notation*:
  - Proximity matrix: $\Delta = (\delta_{ij})$
  - Number of lower off-diagonal elements: $m$
  - $m = \left\lfloor \frac{n}{2} \right\rfloor = \frac{1}{2} n(n-1)$
  - Excludes duplicates (taking only one of $\delta_{ij}$ and $\delta_{ji}$) and diagonal elements ($\delta_{ii}$)
:::

::::

## Example proximity matrix: distances between SA cities

```{r}
#| results: asis
#| fig-align: center
knitr::include_graphics(
  projr::projr_path_get("data-raw-img", "city_distance.png")
)
```

## Common (dis)similarity measures

:::: {.columns}

::: {.column width="50%"}
### Dissimilarity measures

- **Minkowski distance**:
  - $\delta_{ij} = \left( \sum_{k=1}^{r} |X_{ik} - X_{jk}|^{p} \right)^{1/p}$
  - $p=1$: city-block/Manhattan distance
  - $p=2$: Euclidean distance
  - $p=\infty$: Chebychev distance
- **Comparing two sequences**:
  - Hamming distance: # of indices with different values in two equal-length sequences
- **Real-world**:
  - Travel time between destinations
  - Difference in time to failure
  - Subjective:
    - Difference in rated quality
:::

::: {.column width="50%"}
### Similarity measures

- **Continuous**:
  - Centred dot product: $(\mathbf{x}_i - \bar{\mathbf{x}})^\prime (\mathbf{x}_j - \bar{\mathbf{x}})$
  - Correlation coefficient: Pearson, Spearman, etc.
- **Binary** (comparing two sets):
  - Jaccard: $|A \cap B| / |A \cup B|$
- **Real-world**:
  - Frequency of signal confusion
:::
::::

## Classical scaling

- **Classical scaling** [@torgerson1952multidimensional;@torgerson1958theory] (references inaccessible via UCT) is a variant of metric MDS that finds the optimal low-rank configuration of points such that their centred inner products match those in the original space as closely possible.
  - It essentially calculates the answer in this direction:
    - Distances -> centred inner products -> low-rank configuration.
  - The motivation for this is that the optimisation problem has a known solution based on the eigendecomposition, thus avoiding iterative optimisation (apart from calculating eigen decomposition).
  - If the distances are Euclidean ($\delta_{ij} = |\mathbf{x}_i - \mathbf{x}_j| = \sqrt{\sum (x_{ik} - x_{jk})^2}$), then the solution matches the principal component solution.

## Procedure for classical scaling: major steps

- We begin with the dissimilarities $\Delta$, and do not have access to the original data matrix $X$.
  - From $\Delta$, we calculate the matrix $B$ of centred dot products, i.e. where $b_{ij} = \mathbf{x}_i^\prime \mathbf{x}_j$, for $\mathbf{x}_i$ and $\mathbf{x}_j$ the $i$-th and $j$-th rows of $X$.
  - We find the rank $t$ matrix $B^*$ whose $L_2$ norm from $B$ is smallest.
  - It essentially calculates the answer in this direction:
    - Distances -> centred inner products -> low-rank configuration by eigen decomposition.
  - The motivation for this is that the optimisation problem has a known solution based on the eigen decomposition, thus avoiding iterative optimisation (in theory).
  - If the distances are Euclidean ($\delta_{ij} = |\mathbf{x}_i - \mathbf{x}_j| = \sqrt{\sum (x_{ik} - x_{jk})^2}$), then the solution matches the principal components applied to the original matrix $X$ – without knowing $X$!

## Obtaining centred inner products from dissimilarities

```{r}
#| results: asis
#| fig-align: center
knitr::include_graphics(
  projr::projr_path_get("data-raw-img", "derivation-dot_from_distance.png")
)
```

## Objective function for classical scaling

We wish to find $n$ $t$-dimensional points, $\mathbf{Y}_1, . . . ,\mathbf{Y}_n  \in \mathbb{R}^t$ such that

$$tr\{(B - B^*)^2\} = \sum_i \sum_j (b_{ij} - b_{ij}^*)^2,$$

where $B^*$ is the rank $t$ matrix of centred inner products of the points.

- When we use Euclidean distances for proximity matrix, this is equivalent to PCA.
- From $\mathbf{B}$, the points are given by

$$\mathbf{Y} = \mathbf{V}_t \symbf{\Lambda}_t^{1/2},$$

where $\mathbf{V}_t$ is the matrix of the first $t$ eigenvectors of $\mathbf{B}$, and $\symbf{\Lambda}_t$ is the diagonal matrix of the first $t$ eigenvalues of $\mathbf{B}$.

## Example of classical scaling from first principles I {.smaller}

:::: {.columns}

::: {.column width=50%}

- Assume that we have measured distances between four cities, saved as $\mathbf{\Delta}$:

```{r}
#| eval: FALSE
#| echo: FALSE
dist_vec <- c(93, 82, 52, 133, 60, 111)
Delta_mat <- matrix(rep(0, 16), nrow = 4)
k <- 1
for (i in 2:4) for (j in seq_len(i-1)) {
  Delta_mat[i, j] <- dist_vec[[k]]
  k <- k + 1
}
Delta_mat <- Delta_mat + t(Delta_mat)
dput(Delta_mat)
```

```{r}
#| echo: TRUE
Delta_mat <- structure(c(0, 93, 82, 133, 93, 0, 52, 60, 82, 52, 0, 111, 133, 
60, 111, 0), dim = c(4L, 4L))
Delta_mat |> signif(2)
```

:::

::: {.column width=50%}

- Calculate $\mathbf{A}$, where $\mathbf{A}_{ij} =-\frac{1}{2}{\delta}_{ij}^2$:

```{r}
#| echo: TRUE
# remember, R by default performs
# element-wise multiplication
A_mat <- -0.5 * Delta_mat^2
A_mat |> signif(3)
```

:::

::::

## Example of classical scaling from first principles II {.smaller}

:::: {.columns}

::: {.column width=50%}

- Calculate $\mathbf{B}=\mathbf{HAH}$, where $\mathbf{H} = \mathbf{I} - \frac{1}{n}\mathbf{1}\mathbf{1}^\prime$:

```{r}
#| echo: TRUE
H <- diag(4) - 1/4 * matrix(1, 4, 4)
B_mat <- H %*% A_mat %*% H
B_mat |> signif(3)
```

:::

::: {.column width=50%}

- Calculate the eigenvalues and eigenvectors of $\mathbf{B}$:

```{r}
#| echo: TRUE
eig_obj <- eigen(B_mat)
```

- Calculate the principal coordinates:

```{r}
#| echo: TRUE
Y_mat <- eig_obj$vectors %*% diag(sqrt(eig_obj$values))
Y_mat |> signif(2)
```

:::

::::

## Example of classical scaling from first principles III {.smaller}

:::: {.columns}

::: {.column width=50%}

- Plot the first two principal coordinates:

```{r}
#| warning: false
#| echo: true
plot_tbl <- Y_mat |>
 tibble::as_tibble() |>
  dplyr::mutate(
    city = c(
      "Kobenhavn", "Arhus",
      "Odense", "Aalborg"
      )
  )
p <- ggplot(plot_tbl, aes(V1, V2)) +
  geom_point(size = 3) +
  ggrepel::geom_text_repel(
    aes(label = city), size = 10)
```

```{r}
#| echo: false
p <- p +
  cowplot::theme_cowplot(
    font_size = 28
    ) +
  labs(
    x = "First principal coordinate",
    y = "Second principal coordinate",
    title = "Classical scaling of Danish coordinates"
  )
```

:::

::: {.column width=50%}

\vspace{1cm}

```{r}
p
```

:::

::::

## Classical scaling algorithm

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{_data_raw/img/alg-classical_scaling.png}
\end{figure}

## Another classical scaling example: distances between SA cities I

First we load our data from the `DataTidy24STA5069Z` package, installing it if it's not available:

```{r}
#| echo: true
if (!requireNamespace("remotes", quietly = TRUE)) {
  install.packages("remotes")
}
if (!requireNamespace("DataTidy24STA5069Z", quietly = TRUE)) {
  remotes::install_github("MiguelRodo/DataTidy24STA5069Z")
}
data("data_tidy_sa_distance", package = "DataTidy24STA5069Z")
```

## Another example of classical scaling from first principles I {.smaller}

:::: {.columns}

::: {.column width=50%}

- Save proximity data as the matrix $\mathbf{\Delta}$:

```{r}
#| echo: TRUE
Delta_mat <- as.matrix(
  data_tidy_sa_distance
  ) / 1e3
Delta_mat[1:3, 1:3] |> signif(2)
```

:::

::: {.column width=50%}

- Calculate $\mathbf{A}$, where $\mathbf{A}_{ij} =-\frac{1}{2}{\delta}_{ij}^2$:

```{r}
#| echo: TRUE
# remember, R by default performs
# element-wise multiplication
A_mat <- -0.5 * Delta_mat^2
A_mat[1:3, 1:3] |> signif(3)
```

:::

::::

## Another example of classical scaling from first principles II {.smaller}

:::: {.columns}

::: {.column width=50%}

- Calculate $\mathbf{B}=\mathbf{HAH}$, where $\mathbf{H} = \mathbf{I} - \frac{1}{n}\mathbf{1}\mathbf{1}^\prime$:

```{r}
#| echo: TRUE
H <- diag(nrow(A_mat)) -
  1/nrow(A_mat) *
  matrix(1, nrow(A_mat), nrow(A_mat))
B_mat <- H %*% A_mat %*% H
B_mat[1:3, 1:3] |> signif(3)
```

:::

::: {.column width=50%}

- Calculate the eigenvalues and eigenvectors of $\mathbf{B}$:

```{r}
#| echo: TRUE
eig_obj <- eigen(B_mat)
```

- Calculate the principal coordinates:

```{r}
#| echo: TRUE
Y_mat <- eig_obj$vectors %*%
  diag(sqrt(eig_obj$values))
Y_mat[1:3, 1:2] |> signif(2)
```

:::

::::

## Another example of classical scaling from first principles III {.smaller}

:::: {.columns}

::: {.column width=50%}

- Plot the first two principal coordinates:

```{r}
#| warning: false
#| echo: true
plot_tbl <- Y_mat |>
 tibble::as_tibble() |>
  dplyr::mutate(
    city = data_tidy_sa_distance |>
      colnames()
  )
p <- ggplot(plot_tbl, aes(V1, V2)) +
  geom_point(size = 3) +
  ggrepel::geom_text_repel(
    aes(label = city), size = 10)
```

```{r}
#| echo: false
p <- p +
  cowplot::theme_cowplot(
    font_size = 28
    ) +
  labs(
    x = "First principal coordinate",
    y = "Second principal coordinate",
    title = "Classical scaling of SA Cities"
  )
```

:::

::: {.column width=50%}

\vspace{1cm}

```{r}
p
```

:::

::::

## Classical scaling versus least-squares scaling

- *Values approximated*:
  - In classical scaling, we approximate centred inner products.
  - In least-squares scaling, we approximate distances directly.
  - Even when the distances are Euclidean, the two methods will not give the same answer [@hastie_etal09].
    - For one thing, note that the centred inner products are related to the squared distances.
- *Flexibility*:
  - Least-squares scaling is more flexible, in that:
    - It can handle non-Euclidean distances (without "breaking" an assumption).
    - It can handle transformed distances (including merely using ranks).
    - We can weight errors in distance approximations differently (e.g. downeight errors of large original distances).
- *Optimisation approach*:
  - Unlike classical scaling, least-squares scaling does not have a known solution based on the eigen decomposition, and so requires iterative optimisation.

## Complete references

