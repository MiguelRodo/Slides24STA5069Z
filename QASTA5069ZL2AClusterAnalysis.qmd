---
title: Q&A for Cluster Analysis
format:
  html:
    embed-resources: true
---

- What is the fundamental goal of cluster analysis techniques?
  - To discover hidden groupings in data by assigning observations into distinct clusters

- In what way is cluster analysis an unsupervised learning task?
  - The group memberships or labels are not known a priori and must be inferred from the data

- How does cluster analysis differ from supervised learning tasks like classification?
  - In supervised learning, the groups or classes are predefined and used to train the model

- What are some real-world applications of cluster analysis mentioned in the slides:
  - First example?
    - Classifying cells into known sub-types like T cells, B cells, etc. based on surface markers or gene expression data
  - Second example?    
    - Identifying distinct consumer segments with similar purchasing patterns or brand preferences for market research
  - Third example?
    - Discovering sub-types or strains of an animal species based on morphological measurements or genetic data

- Aside from clustering observations, what are some other common unsupervised learning tasks:
  - First task?
    - Feature extraction - deriving new, more informative features from the original input variables
  - Second task?
    - Dimensionality reduction - finding a lower-dimensional representation of the data that preserves the most important information

- What are some equivalent terms for cluster analysis used in different fields:
  - First term?
    - Data segmentation - commonly used in business and marketing applications
  - Second term?
    - Class discovery - often used in scientific contexts like biology or genetics

- How does the formal definition of a cluster in the slides characterize the items within each cluster?
  - Items within a cluster should be close or similar to a central point that serves as the cluster prototype or representative  

- What does the formal definition state about the relationship between items belonging to different clusters?  
  - Items in separate clusters should be far apart or highly dissimilar from each other relative to items in the same cluster

- What are four different conceptual perspectives on what constitutes a cluster:
  - First perspective?
    - Proximity-based: clusters contain points that are close together in the input space based on a dissimilarity or distance measure
  - Second perspective?  
    - Probability-based: clusters correspond to points that likely originate from the same underlying probability distribution
  - Third perspective?
    - Modality-based: clusters are regions of high density in the data space separated by areas of low density
  - Fourth perspective?
    - Density-based: clusters are contiguous high-density regions in the input space, regardless of shape

- What are the two main categories of clustering algorithms covered in the slides:
  - First category?
    - Hierarchical clustering methods - produce a tree-like structure of nested clusters
  - Second category?
    - Partitioning clustering methods - directly divide the data into non-overlapping clusters

- What distinguishes the two primary approaches to hierarchical clustering:
  - First approach?
    - Agglomerative clustering
  - How does agglomerative clustering work:
    - First step?
      - Starts with each observation as a singleton cluster
    - Then what does it repeatedly do?  
      - Merges the two most similar clusters until all observations belong to a single cluster
  - Second approach? 
    - Divisive clustering
  - How does divisive clustering work:
    - First step?
      - Starts with a single cluster containing all observations 
    - Then what does it repeatedly do?
      - Divides existing clusters into smaller clusters until each observation is a singleton

- What are some specific algorithms that fall under the partitioning clustering umbrella:
  - First group of methods?
    - Distance-based partitioning:
      - First algorithm?
        - k-means: finds cluster centroids that minimize within-cluster squared Euclidean distances
      - Second algorithm?  
        - PAM (partitioning around medoids): uses actual data points (medoids) as cluster centers instead of centroids
      - Third algorithm?
        - FANNY (fuzzy analysis): allows observations to have partial membership in multiple clusters
  - Second group of methods?
    - Probability-based partitioning:
      - What probabilistic model is commonly used?
        - Gaussian mixture models: assumes data is generated by a mixture of multivariate normal distributions
      - What does each mixture component represent?  
        - Each mixture component represents a cluster

- What is the most common visual representation of the results of hierarchical clustering?
  - A dendrogram or tree diagram showing the nested structure of the clusters
  
- How does one interpret the height of the branch points (linkages) in a dendrogram?
  - What does the height represent? 
    - The level of dissimilarity between the clusters being joined
  - What does a lower height indicate about the clusters?
    - Clusters joined at lower heights are more similar to each other

- How can one obtain a flat clustering (a partition) from the dendrogram?
  - How do you determine the number of clusters?
    - By cutting the dendrogram at a specific height to produce the desired number of clusters
  - How are observations assigned to clusters based on the cut?  
    - Observations below each cut point are assigned to the same cluster

- What quantity does the agglomerative coefficient measure? 
  - It measures the overall strength or fidelity of the hierarchical clustering tree
- How is the agglomerative coefficient defined?  
  - Defined as the average normalized height of each linkage in the dendrogram
- How do you interpret the agglomerative coefficient?
  - Higher values (close to 1) indicate more distinct and well-separated clusters

- What are the two key steps in divisive hierarchical clustering algorithms:
  - First step?
    - Identifying a splinter group of observations to separate from the remainder at each iteration
  - Second step?
    - Recursively applying the splitting procedure to the splinter group and remainder until all observations are singletons

- What is the defining property of combinatorial clustering algorithms?
  - They use a many-to-one mapping function to assign each observation to a single cluster so each observation belongs to exactly one cluster (hard assignment)

- How do combinatorial algorithms determine the optimal cluster assignment mapping?
  - What is minimized to determine the clustering?
    - A loss function that quantifies the quality or coherence of the resulting clusters 
  - What does the loss function typically measure?
    - The within-cluster dissimilarity or the between-cluster separation

- In contrast to the centroid-based k-means algorithm, what does PAM use as cluster representatives?
  - PAM uses medoids, which are actual data points that minimize the total within-cluster dissimilarity

- What advantages does using medoids offer compared to centroids in k-means:
  - First advantage?
    - Medoids allow the use of arbitrary dissimilarity functions beyond just squared Euclidean distance
  - Second advantage?  
    - Medoids are less sensitive to extreme values and more interpretable as actual observations

- How is the silhouette width used to assess the quality of a clustering assignment for an individual observation?
  - What does a silhouette width close to 1 indicate?
    - A high silhouette width (close to 1) indicates the observation is much closer to its own cluster than others 
  - What does a silhouette width close to -1 suggest?
    - A low silhouette width (close to -1) suggests the observation is poorly matched and could belong to another cluster

- What are the three steps to calculate the silhouette width s(i) for an observation i:
  - First step?
    - Calculate a(i) = the average dissimilarity of i to all other observations in the same cluster
  - Second step?
    - Calculate b(i) = the minimum average dissimilarity of i to observations in each other cluster (i.e., the dissimilarity to the nearest neighboring cluster)
  - Third step?
    - Compute the silhouette width as: s(i) = [b(i) - a(i)] / max[a(i), b(i)]
- What is the range of possible values for the silhouette width?    
  - Silhouette widths lie between -1 and 1, with higher values indicating a better match to the assigned cluster

- What is the key distinction between fuzzy clustering algorithms like FANNY and hard clustering methods?
  - How does fuzzy clustering assign observations to clusters?
    - Fuzzy clustering allows observations to belong to multiple clusters simultaneously with fractional membership weights
  - In contrast, how does hard clustering assign observations?
    - Hard clustering assigns each observation to a single cluster with full membership

- How do fuzzy clustering algorithms represent the degree of cluster membership for each observation:
  - What values do the membership coefficients take?
    - They estimate membership coefficients between 0 and 1 for each observation-cluster pair
  - What is the sum of an observation's membership coefficients across clusters?
    - The membership coefficients for an observation across all clusters sum to 1
  - What does a higher membership coefficient indicate?  
    - Higher membership coefficients indicate stronger association with the corresponding cluster

- For very large datasets, what computational challenge does the CLARA algorithm address?
  - What expensive step does CLARA avoid?
    - CLARA avoids applying expensive clustering operations to the entire dataset
  - How does CLARA efficiently approximate the clustering?
    - It efficiently derives an approximate clustering by repeatedly sampling the data and applying PAM to the samples
  - How is the final clustering obtained?  
    - The final clustering is obtained by assigning all observations to the medoids of the best sample clustering

- What is the core assumption underlying model-based clustering approaches?
  - What kind of statistical model is the data assumed to be generated by? 
    - The data is assumed to be generated by a statistical model consisting of a mixture of probability distributions
  - How are the mixture components related to clusters?
    - Each mixture component is associated with a unique cluster
  - What specific type of mixture model is commonly employed?
    - Commonly, a mixture of multivariate Gaussian (normal) distributions is used

- Under the Gaussian mixture model framework, how is each cluster mathematically represented:
  - What parameterizes each Gaussian mixture component?
    - Each cluster corresponds to a single multivariate Gaussian component in the mixture, parameterized by a mean vector (center) and a covariance matrix (shape and orientation)
  - How are observations assigned to clusters in this framework?  
    - Observations are assigned to the cluster (component) with the highest posterior probability given the data